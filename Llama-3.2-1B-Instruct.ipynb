{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "225ad28cc0c70ef9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:00:36.023802Z",
     "start_time": "2024-11-01T06:00:32.612238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Hugging Face\n",
    "import huggingface_hub\n",
    "import transformers\n",
    "\n",
    "# MLX\n",
    "import mlx_lm\n",
    "import coremltools as ct"
   ],
   "id": "8a90e16a413b2796",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.5.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.4.0 is the most recent version that has been tested.\n",
      "/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/anaconda3/envs/mlx/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/mlx/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/mlx/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Fail to import BlobReader from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n",
      "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
      "Fail to import BlobWriter from libmilstoragepython. No module named 'coremltools.libmilstoragepython'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hugging Face",
   "id": "7a24fce6fdecd3db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:00:36.429790Z",
     "start_time": "2024-11-01T06:00:36.025060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "huggingface_hub.login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "id": "877622f1853ca9f5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "3384a868e2700745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:00:36.433903Z",
     "start_time": "2024-11-01T06:00:36.431176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "#mlx_model_id = \"mlx-community/Llama-3.2-1B-Instruct-4bit\""
   ],
   "id": "4c53d2c9f66f181f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:00:39.599102Z",
     "start_time": "2024-11-01T06:00:36.435508Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)",
   "id": "b19a25bd13f36e9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e19c3dfa3e34f5099f05a9277fb5bc6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cd1597fb6f74442a2ffacd7a41cc3a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e026b23d25274c018c62fffadf862cae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:04:22.562614Z",
     "start_time": "2024-11-01T06:00:39.600374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    return_dict=False,\n",
    "    torchscript=True,\n",
    ").eval()"
   ],
   "id": "b14cdd76f8b31e1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce52996d1b3042bfab27b78445e3b273"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9032580603844919b01e8c3d96bd4622"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41c3bae096284a02b85d6c293e0998dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:04:25.054736Z",
     "start_time": "2024-11-01T06:04:22.564910Z"
    }
   },
   "cell_type": "code",
   "source": "optimized_model = mlx_lm.load(model_id)",
   "id": "10db928c7d191b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "652fe6afb040454d9e97bfa5d01adb55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LICENSE.txt:   0%|          | 0.00/7.71k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10105445dee54c538d13503530bc73d8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "original/params.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36c6ee8e2e084deb826450234877cb2b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "b4b357bd78026be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T06:04:32.876553Z",
     "start_time": "2024-11-01T06:04:32.390771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Hello\"\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    [prompt],\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,  # token sequence length\n",
    "    padding=\"max_length\",\n",
    ")"
   ],
   "id": "d400b93907603ee0",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHello\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m tokenized \u001B[38;5;241m=\u001B[39m tokenizer(\n\u001B[1;32m      4\u001B[0m     [prompt],\n\u001B[1;32m      5\u001B[0m     return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m,  \u001B[38;5;66;03m# token sequence length\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3021\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.__call__\u001B[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   3019\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_in_target_context_manager:\n\u001B[1;32m   3020\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_input_mode()\n\u001B[0;32m-> 3021\u001B[0m     encodings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_one(text\u001B[38;5;241m=\u001B[39mtext, text_pair\u001B[38;5;241m=\u001B[39mtext_pair, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mall_kwargs)\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3023\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_switch_to_target_mode()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3109\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._call_one\u001B[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3104\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   3105\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch length of `text`: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not match batch length of `text_pair`:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3106\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(text_pair)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3107\u001B[0m         )\n\u001B[1;32m   3108\u001B[0m     batch_text_or_text_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(text, text_pair)) \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m text\n\u001B[0;32m-> 3109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_encode_plus(\n\u001B[1;32m   3110\u001B[0m         batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   3111\u001B[0m         add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[1;32m   3112\u001B[0m         padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3113\u001B[0m         truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   3114\u001B[0m         max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   3115\u001B[0m         stride\u001B[38;5;241m=\u001B[39mstride,\n\u001B[1;32m   3116\u001B[0m         is_split_into_words\u001B[38;5;241m=\u001B[39mis_split_into_words,\n\u001B[1;32m   3117\u001B[0m         pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   3118\u001B[0m         padding_side\u001B[38;5;241m=\u001B[39mpadding_side,\n\u001B[1;32m   3119\u001B[0m         return_tensors\u001B[38;5;241m=\u001B[39mreturn_tensors,\n\u001B[1;32m   3120\u001B[0m         return_token_type_ids\u001B[38;5;241m=\u001B[39mreturn_token_type_ids,\n\u001B[1;32m   3121\u001B[0m         return_attention_mask\u001B[38;5;241m=\u001B[39mreturn_attention_mask,\n\u001B[1;32m   3122\u001B[0m         return_overflowing_tokens\u001B[38;5;241m=\u001B[39mreturn_overflowing_tokens,\n\u001B[1;32m   3123\u001B[0m         return_special_tokens_mask\u001B[38;5;241m=\u001B[39mreturn_special_tokens_mask,\n\u001B[1;32m   3124\u001B[0m         return_offsets_mapping\u001B[38;5;241m=\u001B[39mreturn_offsets_mapping,\n\u001B[1;32m   3125\u001B[0m         return_length\u001B[38;5;241m=\u001B[39mreturn_length,\n\u001B[1;32m   3126\u001B[0m         verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   3127\u001B[0m         split_special_tokens\u001B[38;5;241m=\u001B[39msplit_special_tokens,\n\u001B[1;32m   3128\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3129\u001B[0m     )\n\u001B[1;32m   3130\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode_plus(\n\u001B[1;32m   3132\u001B[0m         text\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m   3133\u001B[0m         text_pair\u001B[38;5;241m=\u001B[39mtext_pair,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3151\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3152\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3302\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001B[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[0m\n\u001B[1;32m   3285\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3286\u001B[0m \u001B[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001B[39;00m\n\u001B[1;32m   3287\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3298\u001B[0m \u001B[38;5;124;03m        details in `encode_plus`).\u001B[39;00m\n\u001B[1;32m   3299\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3301\u001B[0m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[0;32m-> 3302\u001B[0m padding_strategy, truncation_strategy, max_length, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_padding_truncation_strategies(\n\u001B[1;32m   3303\u001B[0m     padding\u001B[38;5;241m=\u001B[39mpadding,\n\u001B[1;32m   3304\u001B[0m     truncation\u001B[38;5;241m=\u001B[39mtruncation,\n\u001B[1;32m   3305\u001B[0m     max_length\u001B[38;5;241m=\u001B[39mmax_length,\n\u001B[1;32m   3306\u001B[0m     pad_to_multiple_of\u001B[38;5;241m=\u001B[39mpad_to_multiple_of,\n\u001B[1;32m   3307\u001B[0m     verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m   3308\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3309\u001B[0m )\n\u001B[1;32m   3311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_encode_plus(\n\u001B[1;32m   3312\u001B[0m     batch_text_or_text_pairs\u001B[38;5;241m=\u001B[39mbatch_text_or_text_pairs,\n\u001B[1;32m   3313\u001B[0m     add_special_tokens\u001B[38;5;241m=\u001B[39madd_special_tokens,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3330\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3331\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2923\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001B[0;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001B[0m\n\u001B[1;32m   2921\u001B[0m \u001B[38;5;66;03m# Test if we have a padding token\u001B[39;00m\n\u001B[1;32m   2922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m):\n\u001B[0;32m-> 2923\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2924\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsking to pad but the tokenizer does not have a padding token. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2925\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2926\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor add a new pad token via `tokenizer.add_special_tokens(\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpad_token\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[PAD]\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m})`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2927\u001B[0m     )\n\u001B[1;32m   2929\u001B[0m \u001B[38;5;66;03m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001B[39;00m\n\u001B[1;32m   2930\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2931\u001B[0m     truncation_strategy \u001B[38;5;241m!=\u001B[39m TruncationStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_TRUNCATE\n\u001B[1;32m   2932\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m padding_strategy \u001B[38;5;241m!=\u001B[39m PaddingStrategy\u001B[38;5;241m.\u001B[39mDO_NOT_PAD\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2935\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (max_length \u001B[38;5;241m%\u001B[39m pad_to_multiple_of \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m   2936\u001B[0m ):\n",
      "\u001B[0;31mValueError\u001B[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T06:04:25.658847Z",
     "start_time": "2024-11-01T06:04:25.057228Z"
    }
   },
   "source": [
    "if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "response = mlx_lm.generate(optimized_model, tokenizer, prompt=prompt, verbose=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Nov 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m     messages \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt}]\n\u001B[1;32m      5\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[1;32m      6\u001B[0m         messages, tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     )\n\u001B[0;32m----> 9\u001B[0m response \u001B[38;5;241m=\u001B[39m mlx_lm\u001B[38;5;241m.\u001B[39mgenerate(optimized_model, tokenizer, prompt\u001B[38;5;241m=\u001B[39mprompt, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/mlx_lm/utils.py:336\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(model, tokenizer, prompt, max_tokens, verbose, formatter, **kwargs)\u001B[0m\n\u001B[1;32m    333\u001B[0m tic \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m    334\u001B[0m detokenizer\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m--> 336\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n, (token, logprobs) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28mrange\u001B[39m(max_tokens),\n\u001B[1;32m    338\u001B[0m     generate_step(prompt_tokens, model, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs),\n\u001B[1;32m    339\u001B[0m ):\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    341\u001B[0m         prompt_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m tic\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/mlx_lm/utils.py:223\u001B[0m, in \u001B[0;36mgenerate_step\u001B[0;34m(prompt, model, temp, repetition_penalty, repetition_context_size, top_p, min_p, min_tokens_to_keep, prefill_step_size, max_kv_size, prompt_cache, logit_bias, logits_processor)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;66;03m# Create the KV cache for generation\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompt_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 223\u001B[0m     prompt_cache \u001B[38;5;241m=\u001B[39m cache\u001B[38;5;241m.\u001B[39mmake_prompt_cache(model, max_kv_size)\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(prompt_cache) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(model\u001B[38;5;241m.\u001B[39mlayers):\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWrong number of layers in the prompt cache.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/mlx/lib/python3.12/site-packages/mlx_lm/models/cache.py:26\u001B[0m, in \u001B[0;36mmake_prompt_cache\u001B[0;34m(model, max_kv_size)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mmake_cache()\n\u001B[0;32m---> 26\u001B[0m num_layers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(model\u001B[38;5;241m.\u001B[39mlayers)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_kv_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     29\u001B[0m         RotatingKVCache(max_size\u001B[38;5;241m=\u001B[39mmax_kv_size, keep\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_layers)\n\u001B[1;32m     30\u001B[0m     ]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'layers'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Core ML",
   "id": "149d3c2524fb5105"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "traced_optimized_model = torch.jit.trace(\n",
    "    optimized_model,\n",
    "    (tokenized[\"input_ids\"], tokenized[\"attention_mask\"])\n",
    ")"
   ],
   "id": "e4cabf8d6d1b7148"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ane_mlpackage_obj = ct.convert(\n",
    "    traced_optimized_model,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[\n",
    "        ct.TensorType(\n",
    "                f\"input_{name}\",\n",
    "                    shape=tensor.shape,\n",
    "                    dtype=np.int32,\n",
    "                ) for name, tensor in tokenized.items()\n",
    "            ],\n",
    "            compute_units=ct.ComputeUnit.ALL,\n",
    ")\n",
    "out_path = \"HuggingFace_ane_transformers_distilbert_seqLen128_batchSize1.mlpackage\"\n",
    "ane_mlpackage_obj.save(out_path)"
   ],
   "id": "c475008a49b1d863"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
